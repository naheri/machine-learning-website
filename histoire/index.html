<!DOCTYPE html>
<html lang="fr">
<head>
  <meta charset="UTF-8">
  <title>histoire</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/meyer-reset/2.0/reset.min.css">
  <link rel="stylesheet" href="../histoire/style.css">
  <link rel="shortcut icon" href="https://upload.wikimedia.org/wikipedia/commons/d/d5/Hey_Machine_Learning_Logo.png" type="image/x-icon">
</head>
<section id="section0">
  <a href="http://localhost:8889/projet20212363/start/index.html">
    <img id=logo-image src="https://upload.wikimedia.org/wikipedia/commons/d/d5/Hey_Machine_Learning_Logo.png" alt="logo">
  </a>
  <nav class="navbar">
    <div class="nav-links"> 
      <ul>
        <li><a href="http://localhost:8889/projet20212363/start/index.html">acceuil</a></li>
        <li><a href="http://localhost:8889/projet20212363/histoire/index.html">histoire</a></li>
        <li><a href="http://localhost:8889/projet20212363/algorithmes/index.html">algorithmes</a></li>
        <li><a href="http://127.0.0.1:5000/">applications</a></li>
        <li><a href="http://localhost:8889/projet20212363/contact/index.html">contact</a></li>
      </ul>
    </div>
    <img class="menu-hamburger" src="https://upload.wikimedia.org/wikipedia/commons/thumb/b/b2/Hamburger_icon.svg/1024px-Hamburger_icon.svg.png" alt="icône menu hamburger">
  </nav>

  <div class="flex-container">
    <div class="text-flex">
      <h1>L'histoire du Machine Learning</h1>
      <p>Depuis longtemps, nous essayons de donner un sens aux données, de les traiter pour en tirer des enseignements et d'automatiser ce processus autant que possible. C'est ainsi qu'est née la technologie que nous appelons aujourd'hui "apprentissage automatique". Attachez votre ceinture et partons pour un voyage fascinant dans l'histoire du machine learning afin de découvrir comment tout a commencé, comment il a évolué pour devenir ce qu'il est aujourd'hui et ce que l'avenir réserve à cette technologie.</p>
    </div>
      <div class="summary">
        <h2>Table des matières</h2>
        <ul>
          <li><a href="#1943">1943 - Premier modèle mathématique d'un neurone biologique</a></li>
          <li><a href="#1949">1949 - Règle de Hebb</a></li>
          <li><a href="#1950">1950 - Test de Turing</a></li>
          <li><a href="#1952">1952 - Le Machine Learning et les échecs</a></li>
          <li><a href="#1956">1956 - Les prémices de l'IA</a></li>
          <li><a href="#1958">1958 - Le Perceptron</a></li>
          <li><a href="#1963">1963 - Morpion</a></li>
          <li><a href="#1965">1965 - 1ers réseaux neuronaux multicouches</a></li>
          <li><a href="#1967">1967 - KNN</a></li>
          <li><a href="#1973">1973 - L'hiver de l'IA</a></li>
          <li><a href="#1979">1979 - Le Neocognitron et le chariot de Stanford</a></li>
          <li><a href="#1981">1981 - Apprentissage basé sur l'expérience</a></li>
          <li><a href="#1989">1989</a></li>
          <li><a href="#1992">1992 - Le Backgammon</a></li>
          <li><a href="#1997">1997 - Deep Blue</a></li>
          <li><a href="#2011">2011 - Watson d'IBM et le Google Brain</a></li>
          <li><a href="#2012">2012 - ImageNet</a></li>
          <li><a href="#2014">2014 - DeepFace & Sibyl</a></li>
          <li><a href="#2015">2015 - Platformisation du ML</a></li>
          <li><a href="#2016">2016 - AlphaGo et Face2Face</a></li>
          <li><a href="#2017">2017 - Waymo</a></li>
          <li><a href="#2018">2018 - AlphaFold</a></li>
          <li><a href="#2020">2020 - GPT-3</a></li>
          <li><a href="#2022">2022 - Le futur du machine learning</a></li>
          <li><a href="#ChatGPT">ChatGPT</a></li>
        </ul>
      </div>
    </div>

  <div class="wrapper">
    <h3>EXPLORER!</h3>
    <a href="#section1">
        <button onclick="tongueSound()">
            <div class="arrow bounce"></div>
        </button>
    </a>
  </div>
  <div class="wrapper">
    <h3>REMONTER DANS LE TEMPS!</h3>
    <a href="#section0">
        <button onclick="plantedSound()">
            <div class="arrow bounce"></div>
        </button>
    </a>
  </div>
</section>
<section id="section1">
  <div class="container">
    <div class="TITLE"><h2>Qu'est-ce que le Machine learning (apprentissage automatique) ?</h2></div>
    <div class="text"><p>Encore confus pour de nombreuses personnes, le Machine Learning est une science moderne permettant de découvrir des répétitions (des patterns) dans un ou plusieurs flux de données et d’en tirer des prédictions en se basant sur des statistiques. En clair, le Machine Learning se base sur le forage de données, 
      permettant la reconnaissance de patterns pour fournir des analyses prédictives.</p></div>
    <div class="illustration">
      <iframe src="https://www.youtube.com/embed/hyYp4bP-dl8?autoplay=1&controls=0&mute=1">
    </iframe>
    </div>
  </div>
</section>
<section id="section2">
  <div class="maincontainer">
    <div class="title"><h2 id="1943">1943 - Premier modèle mathématique d'un neurone biologique</h2></div>
    <div class="text"><p>Le premier modèle mathématique des réseaux neuronaux a été développé par Walter Pitts et Warren McCulloch en 1943. Les algorithmes ont été développés sur la base de leur publication savante, "A Logical Calculus of the Ideas Immanent in Nervous Activity".
      Ce neurone de McCulloch Pitts n'a aucun mécanisme d'apprentissage et une capacité très faible. Cependant, il a servi de véritable impulsion pour le développement du machine learning en tant que domaine à l'époque moderne et a ensuite ouvert la voie à le Deep Learning et à l'apprentissage automatique quantique.</p></div>
    <div class="illustration"></div>
    <div class="title2"><h2 id="1949">1949 - Règle de Hebb</h2></div>
    <div class="text2"><p>Donald O. Hebb, un psychologue canadien, a écrit "The Organization of Behavior : Une théorie neuropsychologique." Hebb y expose une théorie sur la manière dont l'excitation des neurones et leur communication entre eux ont influencé la façon dont les psychologues ont perçu le traitement des stimuli par l'esprit.
      Cette idée a été appliquée à l'origine à la recherche sur la façon dont le cerveau apprend. En outre, elle a ouvert la voie à la création de dispositifs informatiques qui imitent les fonctions neuronales du monde réel, comme l'apprentissage automatique.</p></div>
    <div class="illustration2"></div>
  </div>
  
</section>
<section id="section3">
  <div class="maincontainer">
    <div class="title"><h2 id="1950">1950 - Le Test de Turing</h2></div>
    <div class="text"><p>En 1950, l'informaticien anglais <a href="https://fr.wikipedia.org/wiki/Alan_Turing" target="_blank" > Alan Turing</a> a proposé le test de Turing pour tester l'intelligence d'une machine. Il sert de référence pour l'intelligence artificielle. Un ordinateur est considéré comme intelligent si une personne ne peut pas distinguer si elle parle à une autre personne ou à un autre ordinateur.
      Le test de Turing a été critiqué à la fois pour sa difficulté à mettre au point un test équitable et fiable et pour sa mesure inadéquate des capacités. Il n'en reste pas moins un tournant crucial dans le développement de l'intelligence artificielle.</p></div>
    <div class="illustration">
      <img src="https://cdn.ttgtmedia.com/rms/onlineImages/crm-turing_test.jpg" alt="test-de-turing">
    </div>
    <div class="title2"><h2 id="1952">1952 - Le Machine Learning et les échecs</h2> </div>
    <div class="text2"><p>Le mathématicien anglais Arthur Samuel a créé un programme d'apprentissage informatique pour jouer aux dames informatiques de niveau championnat, qui a été créé pour jouer sur l'IBM 701. Il a initié l'élagage alpha-bêta, une conception qui mesure les chances de victoire de chaque camp.
      Ce programme informatique choisit son prochain coup en utilisant un algorithme minimax, qui calcule le meilleur coup possible pour un joueur dans un jeu en minimisant le gain maximal de l'adversaire et en maximisant le gain minimal du joueur.
      Arthur Samuel est la première personne à avoir créé et popularisé le terme "machine learning".</p></div>
    <div class="illustration2">
      <img src="https://images.chesscomfiles.com/uploads/game-gifs/90px/brown/neo/0/cc/0/0/bUMwS2d2NVFmSDl6a3N6R2JxR1BlZyFUbEI4IUJLVENkSkNJcUFRMEpkWVFIeUl5ZHlQWUFSWVJLUjBKeUUxTEV4N1RzQUpQQUlQSmNNVGpmZVhQTTA5OHhMMlVMTWoydkszVk1GVk5hZFBJRk0hM2RKUUpuRDJWTVQ4IUt2VjJURiE4REwzIUxUMj9GTSEzdks_IVQxNlgxfTghZWY0OTA5ITI5MklBTVUzIWY5.gif" alt="magnus-carlsen-vs-stockfish">
      <p style="overflow-wrap: anywhere;"><a href="https://www.chess.com/players/magnus-carlsen" target="_blank">Magnus Carlsen</a> VS <a href="https://fr.wikipedia.org/wiki/Stockfish_(programme_d%27%C3%A9checs)" target="_blank">Stockfish</a></p>
    </div>
  </div>
</section>
<section id="section4">
  <div class="maincontainer">
    <div class="title">
      <h2 id="1956">1956 - Les prémices de l'intelligence artifcielle</h2>
    </div>
    <div class="text">
      <p>Dans l'histoire du machine learning, l'atelier de Dartmouth en 1956 est largement considéré comme l'événement fondateur de l'intelligence artificielle en tant que domaine. L'informaticien John McCarthy a invité des mathématiciens, des scientifiques et des chercheurs de renom à un atelier de six à huit semaines. Ils se sont réunis au Dartmouth College pour établir et réfléchir aux domaines de recherche de l'IA et de la ML.</p>
    </div>
    <div class="illustration">
      <img src="https://www.startechup.com/wp-content/uploads/Dartmouth-college-workshop-1956-1536x1216.jpeg.webp" alt="atelier-de-darmouth">
    </div>
    <div class="title2">
      <h2 id="1958">1958 - Le Perceptron</h2>
    </div>
    <div class="text2">
      <p>
        Le psychologue Frank Rosenblatt a tenté de construire "la première machine capable de produire une idée originale" et a ensuite conçu le Perceptron, premier réseau neuronal jamais produit.
Il a combiné le modèle d'interaction des cellules cérébrales de Donald Hebb avec les efforts de machine learning d'Arthur Samuel. Le Perceptron a été alimenté par une série de cartes perforées et, après 50 essais, il a appris à distinguer les cartes comportant des marques à gauche des marques à droite.
      </p>
    </div>
    <div class="illustration2">
      <img src="https://media.lesechos.com/api/v1/images/view/5bf3e6443e454627390c6d24/1280x720/2108927-1957-le-perceptron-premiere-machine-apprenante-web-tete-030504649337.jpg" alt="perceptron">
    </div>
  </div>
</section>
<section id="section5">
  <div class="maincontainer">
    <div class="title"><h2 id="1963">1963 - Morpion</h2></div>
    <div class="text"><p>L'informaticien Donald Michel a conçu le Machine Educable Noughts And Crosses Engine (MENACE), une grande pile de boîtes d'allumettes contenant plusieurs perles et utilisant l'apprentissage par renforcement pour jouer au morpion.
      MENACE fonctionne un peu comme un réseau neuronal. Au départ, il est optimisé de manière aléatoire, mais après avoir joué quelques parties, il s'ajuste pour favoriser les stratégies gagnantes dans chaque situation.
      Vous pouvez jouer contre MENACE <a href="https://www.mscroggs.co.uk/menace/">ici</a>.</p>
    </div>
    <div class="illustration">
      <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/3/32/Tic_tac_toe.svg/1200px-Tic_tac_toe.svg.png" alt="morpion-image">
    </div>
    <div class="title2">
      <h2 id="1965">1965 - 1ers réseaux neuronaux multicouches</h2>
    </div>
    <div class="text2">
      <p>Alexey (Oleksii) Ivakhnenko et Valentin Lapa sont des scientifiques qui ont travaillé ensemble pour développer le tout premier perceptron multicouche. Il s'agit d'une représentation hiérarchique d'un réseau neuronal qui utilise une fonction d'activation polynomiale et qui est entraîné à l'aide de la méthode de groupe de traitement des données (GMDH).
      Ivakhnenko est souvent considéré comme le père du <a href="https://datascientest.com/deep-learning-definition">Deep Learning</a>.</p>
    </div>
    <div class="illustration2">
      <iframe src="https://www.youtube.com/embed/7YaqzpitBXw?controls=0" title="What are MLPs (Multilayer Perceptrons)?" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    </div>
  </div>
</section>
<section id="section6">
  <div class="maincontainer">
    <div class="title">
      <h2 id="1967">1967 - L'algorithme des voisins les plus proches</h2>
    </div>
    <div class="text">
      <p>Thomas Cover et Peter Hart ont publié sa "Nearest Neighbor Pattern Classification" en 1967. Cet ouvrage a jeté les bases de la reconnaissance des formes et de la régression dans l'apprentissage automatique.
        <a href="../algorithmes/index.html#section10">L'algorithme du plus proche voisin</a> est une méthode de reconnaissance des formes très élémentaire qui a été développée pour permettre aux ordinateurs de procéder à une détection rudimentaire des formes. Il fonctionne en comparant des données existantes et en les classant comme le plus proche voisin, c'est-à-dire l'élément le plus similaire en mémoire, ce qui peut aider les vendeurs de voyages dans une ville aléatoire.</p>
    </div>
    <div class="illustration">
      <img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2018/03/knn3.png" alt="knn-schéma">
    </div>
    <div class="title2">
      <h2>1973 - L'hiver de l'IA</h2>
    </div>
    <div class="text2">
      <p>Le rapport Lighthill, rédigé par James Lighthill en 1973, présentait des prévisions très pessimistes quant au développement des aspects fondamentaux de la recherche sur l'IA, déclarant : "Dans aucune partie du domaine, les découvertes faites jusqu'à présent n'ont produit l'impact majeur qui était alors promis." En conséquence, le gouvernement britannique a réduit le financement de la recherche sur l'IA dans toutes les universités sauf deux. C'est une partie de l'histoire du machine learning connue sous le nom d'hiver de l'IA.</p>
    </div>
    <div class="illustration2"></div>
  </div>
</section>
<section id="section7">
  <div class="maincontainer">
    <div class="title">
      <h2 id="1979">1979 - Le Neocognitron et le chariot de Stanford</h2>
    </div>
    <div class="text">
      <p>L'informaticien japonais Kunihiko Fukushima publie ses travaux sur le Neocognitron, un réseau hiérarchique multicouche utilisé pour détecter des motifs et inspirer les réseaux neuronaux convolutifs utilisés pour analyser les images. Ces travaux ont déclenché une révolution dans ce que nous appelons aujourd'hui l'IA (intelligence artifcielle).
      </p>
    </div>
    <div class="illustration">
      <img src="https://www.startechup.com/wp-content/uploads/Stanford-cart-1979.jpeg.webp" alt="chariot de stanford">
    </div>
    <div class="title2">
      <h2>1981 - Apprentissage basé sur l'explication</h2>
    </div>
    <div class="text2">
      <p> Cette année-là, Gerald Dejong a présenté le concept d'apprentissage par l'explication (EBL), dans lequel un ordinateur analyse les données d'apprentissage et crée une règle générale qu'il peut suivre en écartant les données sans importance. Par exemple, si l'on demande au logiciel de se concentrer sur la reine aux échecs, il éliminera toutes les pièces n'ayant pas d'effet immédiat. Cette méthode a jeté les bases des techniques modernes d'apprentissage supervisé.</p>
    </div>
    <div class="illustration2"></div>
  </div>
</section>
<section id="section8">
  <div class="maincontainer">
    <div class="title">
      <h2 id="1989">1989</h2>
    </div>
    <div class="text">
      <p>Le concept de boosting a été présenté pour la première fois en 1990 dans un article intitulé "The Strength of Weak Learnability" par Robert Schapire et Yoav Freund. Il a marqué un développement nécessaire pour l'évolution du machine learning.
        Comme l'indique Schapire, "un ensemble d'apprenants faibles peut créer un seul apprenant fort". Cela se traduit simplement par la production de nombreux modèles plus faibles et la combinaison de leurs prédictions pour les convertir en un seul modèle puissant.</p>
    </div>
    <div class="illustration"></div>
    <div class="title2">
      <h2 id="1992">1992 - Le Backgammon</h2>
    </div>
    <div class="text2">
      <p>Le chercheur Gerald Tesauro a créé un programme basé sur un réseau neuronal artificiel capable de jouer au backgammon avec des capacités égalant celles des meilleurs joueurs humains. Le logiciel de jeu de backgammon, appelé TD-Gammon était capable de jouer à un haut niveau après seulement quelques heures d'entraînement, et il a continué à s'améliorer au fil des parties.
        Le succès du programme a constitué une étape importante dans l'intelligence artificielle et l'histoire du machine learning, car il a montré que les réseaux neuronaux pouvaient être utilisés pour créer des programmes capables d'apprendre et de s'améliorer par l'expérience.
        </p>
    </div>
    <div class="illustration2">
      <img src="https://images6.alphacoders.com/855/thumb-1920-855130.jpg" alt="image backgammon">
    </div>
  </div>
</section>
<section id="section9">
  <div class="maincontainer">
    <div class="title">
      <h2 id="1997">
        1997 - DeepBlue
      </h2>
    </div>
    <div class="text">
      <p>En 1997, Deep Blue, un superordinateur spécialisé dans le jeu d'échecs développé par IBM, est devenu le premier système informatique à battre un champion du monde en titre, en battant Garry Kasparov.
        C'est également l'année où Sepp Hochreiter et Jürgen Schmidhuber ont publié un article révolutionnaire sur la "mémoire à long terme" (LSTM). Il s'agit d'une architecture de réseau neuronal récurrent qui va révolutionner le Deep Learning dans les décennies à venir.
      </p>
    </div>
    <div class="illustration">
      <img src="https://www.ballecourbe.ca/wp-content/uploads/2021/02/Untitled-4-4-960x540.jpg" alt="kasparov vs deepblue">
    </div>
    <div class="title2">
      <h2 id="2011">
        2011 - Watson d'IBM et le Google Brain
      </h2>
    </div>
    <div class="text2">
      <p>Watson est un système cognitif alimenté par l'intelligence artificielle et le traitement du langage naturel développé par IBM. En 2011, Watson a participé au jeu télévisé 'Jeopardy !' contre deux concurrents humains et a gagné. Il est ainsi devenu le premier système informatique à gagner un jeu télévisé contre des humains.
        La même année, l'équipe du X Lab de Google a développé un algorithme de machine learning 'Google Brain'. L'objectif étant de créer un réseau neuronal profond capable d'apprendre à parcourir de manière autonome des vidéos YouTube et à reconnaître des chats dans des images.
      </p>
    </div>
    <div class="illustration2">
      <img src="https://theblogisright.files.wordpress.com/2011/02/9bd1e-fullscreencapture218201142918pm-bmp.jpg" alt="watson au jeu télévisé">
    </div>
  </div>
</section>
<section id="section10">
  <div class="maincontainer">
    <div class="title">
      <h2 id="2012">2012 - ImageNet</h2>
    </div>
    <div class="text">
      <p>En 2012, Alex Krizhevsky, Geoffrey Hinton et Ilya Sutskever ont publié un article de recherche détaillant un modèle capable de réduire à pas de géant le taux d'erreur des systèmes de reconnaissance d'images.
        AlexNet, un modèle CNN basé sur GPU créé par Alex Krizhevsky, a remporté le concours de classification d'images d'Imagenet avec une précision de 84 %. Il a considérablement amélioré le taux de réussite de 75 % des modèles précédents. Cette victoire marque le début d'une révolution deep learning qui va s'étendre au monde entier.</p>
    </div>
    <div class="illustration">

    </div>
    <div class="title2">
      <h2 id="2014">2014 - DeepFace & Sibyl</h2>
    </div>
    <div class="text2">
      <p>Facebook a développé DeepFace, un algorithme de Deep Learning qui peut reconnaître des individus sur des photos. Capable d'identifier des visages humains avec une précision de 97,35 %, cette réalisation historique dans le domaine des algorithmes de reconnaissance faciale aura un impact profond sur la capacité de Facebook à assurer la sécurité des données des utilisateurs et à lutter contre la criminalité.
        Une autre étape importante dans l'histoire du machine learning est la mise à disposition du public de Sibyl, un système de machine learning à grande échelle de Google. Ce système comprend des algorithmes sophistiqués permettant de prédire le comportement des utilisateurs.</p>
    </div>
    <div class="illustration2">

    </div>
  </div>
</section>
<section id="section11">
  <div class="maincontainer">
    <div class="title">
      <h2 id="2015">2015 - Platformisation du Machine Learning</h2>
    </div>
    <div class="text">
      <p>Amazon lance sa propre plateforme de machine learning. Le géant du commerce électronique rend le machine learning accessible à toute personne possédant un compte Amazon Web Services (AWS). La plateforme fournit un ensemble d'outils et d'algorithmes permettant aux data scientists de construire et d'entraîner des modèles.
        Microsoft avait également développé le Distributed Machine Learning Toolkit, qui permettait de partager efficacement les problèmes de machine learning entre plusieurs ordinateurs.</p>
    </div>
    <div class="illustration"></div>
    <div class="title2">
      <h2 id="2016">2016 – AlphaGo et Face2Face</h2>
    </div>
    <div class="text2">
      <p>Le go est un ancien jeu de société chinois avec tellement de mouvements possibles à chaque étape que les positions futures sont difficiles à prévoir. Lorsque l'algorithme AlphaGo s'est développé en mars 2016, il a choqué le monde en battant l'un des meilleurs joueurs de Go, Lee Sedol.
        Toujours en 2016, une équipe de scientifiques a dévoilé Face2Face lors de la Conférence sur la vision informatique et la reconnaissance des formes. La plupart des logiciels "DeepFake" sont aujourd'hui basés sur son cadre et ses algorithmes.</p>
    </div>
    <div class="illustration2">
      <img src="https://ca-times.brightspotcdn.com/dims4/default/4f7d540/2147483647/strip/true/crop/2048x1327+0+0/resize/1200x778!/quality/80/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F3a%2F55%2F4b4a54760c2294a30682cb1e7c74%2Fla-fg-la-fg-0312-south-korea-go-2016031-001" alt="alphago vs lee seedol">
    </div>
  </div>
</section>
<section id="section12">
  <div class="maincontainer">
    <div class="title">
      <h2 id="2017">2017 - Waymo</h2>
    </div>
    <div class="text">
      <p>Waymo est devenue la première entreprise de voitures à conduite autonome à fonctionner 
        sans intervention humaine. Les véhicules de l'entreprise ont maintenant parcouru plus de 
        5 millions de kilomètres sur des routes publiques, les conducteurs humains n'intervenant
         que lorsque cela est nécessaire.
      </p>
    </div>
    <div class="illustration">
      <img src="https://media-cldnry.s-nbcnews.com/image/upload/newscms/2019_11/2785531/190314-waymo-rideshare-mc-11142.JPG" alt="voiture waymo">
    </div>
    <div class="title2">
      <h2 id="2018">2018 - AlphaFold</h2>
    </div>
    <div class="text2">
      <p>Après avoir créé AlphaGo, l'équipe a fait le premier pas dans le développement d'algorithmes pour des problèmes exactement comme le repliement des protéines. AlphaFold a été construit pour prédire les formes 3D des protéines, les molécules fondamentales de la vie. Elle a entraîné un réseau neuronal sur des milliers de protéines connues jusqu'à ce qu'il puisse prédire de manière indépendante les structures 3D à partir des acides aminés. Finalement, il l'utilise pour prédire les distances entre les paires d'acides aminés et les angles entre les liaisons chimiques qui les relient.</p>
    </div>
    <div class="illustration2">
      <img src="https://lh3.googleusercontent.com/J5RC0j1DeUWaNc5h6nGwJwsQSLUuTXINP6we2ymLJ_WUg9bH-hvfvI8WVFeghN-_YR69MryNK5O2rFcVNwz9PZePpBtLdwdshCGzLdM=w2048-rw-v1" alt="protéines">
    </div>
  </div>
</section>
<section id="section13">
  <div class="maincontainer">
    <div class="title">
      <h2 id="2020">2020 - GPT-3 et la montée en puissance du no-code</h2>
    </div>
    <div class="text">
      <p>
        Lorsque le monde était aux prises avec la pandémie en 2020, <a href="https://openai.com/">OpenAI</a> a créé un algorithme d'intelligence artificielle, GPT-3, capable de générer un texte de type humain. En son temps, il s'agit du modèle de langage le plus avancé au monde, utilisant 175 milliards de paramètres et le superordinateur d'IA de Microsoft Azure pour l'entraînement.
        En dehors de cela, Zapier découvre une énorme augmentation de l'utilisation d'outils d'IA sans code ou à faible code à partir du début de 2020. Parmi les plateformes d'IA sans code les plus populaires, citons AutoML de Google, SageMaker d'Amazon et Azure ML de Microsoft. Elles permettent aux utilisateurs n'ayant aucune expérience du codage de former et de déployer des algorithmes d'apprentissage automatique. Ce mouvement est encouragé par la demande des entreprises de produire des applications d'IA rapidement et sans coût supplémentaire.
      </p>
    </div>
    <div class="illustration"></div>
    <div class="title2">
      <h2 id="2022">2022 - Le futur du machine learning</h2>
    </div>
    <div class="text2">
      <p>Le machine learning évolue à un rythme étonnant et ne montre aucun signe de ralentissement. Au début de l'année 2022, nous l'avons déjà vu progresser encore davantage avec l'essor des applications logicielles de services cognitifs. L'apprentissage automatique a même été défini par l'université de Stanford comme "la science qui consiste à amener les ordinateurs à agir sans être explicitement programmés."</p>
    </div>
    <div class="illustration2">
      <img src="https://www.oreilly.com/library/view/programming-ai/9781492037624/assets/pai_0101.png" alt="machine learning">
    </div>
  </div>
</section>
<section id="section14">
  <div class="last-container">
    <div class="title">
      <h2 id="ChatGPT">ChatGPT</h2>
    </div>
    <div class="text">
      <p>La nouvelle Intelligence Artificielle qui fait du bruit en ce moment est celle crée par OpenAI et basé sur GPT-3, ouverte au public depuis le 30 novembre 2022 : <a href="https://chat.openai.com/chat">ChatGPT.</a> Qu'est ce que ça veut dire concrètement ? Posons lui la question</p>
    </div>
    <div class="cell1">
      <img src="../images/screen1ChatGPT.png" alt="demandeChatGPT">
    </div>
    <div class="cell2">
      <img src="../images/screen2ChatGPT.png" alt="demande2ChatGPT">
    </div>
    <div class="cell3">
      <img src="../images/screen3ChatGPT.png" alt="demande3ChatGPT">
    </div>
  </div>
  
</section>
<script src='https://cdnjs.cloudflare.com/ajax/libs/randomcolor/0.5.2/randomColor.js'></script>
<script src='https://bundle.run/css-scroll-snap-polyfill@0.1.2'></script>
<script src="../histoire/script.js"></script>
</body>

</html>
